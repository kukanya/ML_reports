{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анастасия Куканова, A3200\n",
    "### Задание 1. Градиентный спуск\n",
    "\n",
    "#### Исходный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from random import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_point(lower, upper):\n",
    "    return np.array(tuple((l + random() * (u - l)) for l, u in zip(lower, upper)))\n",
    "\n",
    "\n",
    "def in_range(point, lower, upper):\n",
    "    return all(tuple(l <= comp <= u for comp, l, u in zip(point, lower, upper)))\n",
    "\n",
    "\n",
    "def constant_step(step):\n",
    "    while True:\n",
    "        yield step\n",
    "\n",
    "\n",
    "def decreasing_step(initial, coefficient):\n",
    "    step = initial\n",
    "    while True:\n",
    "        yield step\n",
    "        step *= coefficient\n",
    "\n",
    "\n",
    "def descent(lower, upper, func, grad, step_gen, *step_args, normalize_grad=False, epsilon=0.01,\n",
    "            print_result=False, return_step_count=False):\n",
    "    step = step_gen(*step_args)\n",
    "    step_count = 1\n",
    "    current_point = generate_point(lower, upper)\n",
    "    if print_result:\n",
    "        print(\"Start: (\", reduce(lambda a, b: \"{}, {}\".format(a, b), current_point), \")\", sep=\"\")\n",
    "    while True:\n",
    "        lam = next(step)\n",
    "        g = grad(current_point)\n",
    "        if normalize_grad:\n",
    "            g /= np.linalg.norm(g)\n",
    "        step_count += 1\n",
    "        while True:\n",
    "            next_point = current_point - lam * g\n",
    "            if in_range(next_point, lower, upper):\n",
    "                break\n",
    "            else:\n",
    "                lam /= 2.0\n",
    "        if step_count > 100000 or np.linalg.norm(current_point - next_point) < epsilon:\n",
    "            if print_result:\n",
    "                print(\"Steps done:\", step_count)\n",
    "                print(\"Result: f(\", reduce(lambda a, b: \"{}, {}\".format(a, b), next_point), \") =  \",\n",
    "                      func(next_point)[0], sep=\"\")\n",
    "            if return_step_count:\n",
    "                return next_point, func(next_point), step_count\n",
    "            else:\n",
    "                return next_point, func(next_point)\n",
    "        else:\n",
    "            current_point = next_point\n",
    "\n",
    "\n",
    "def descent_optimal_step(lower, upper, func, grad, normalize_grad=False, epsilon=0.01,\n",
    "                         print_result=False, return_step_count=False):\n",
    "    step_count = 1\n",
    "    dich_step_count = 0\n",
    "    current_point = generate_point(lower, upper)\n",
    "    if print_result:\n",
    "        print(\"Start: (\", reduce(lambda a, b: \"{}, {}\".format(a, b), current_point), \")\", sep=\"\")\n",
    "    lam_max = np.linalg.norm(upper - lower)\n",
    "    while True:\n",
    "        g = grad(current_point)\n",
    "        if normalize_grad:\n",
    "            g /= np.linalg.norm(g)\n",
    "        step_count += 1\n",
    "        while True:\n",
    "            lam, dsc = dichotomy(np.array([0]), np.array([lam_max]), lambda lam_: func(current_point - lam_ * g),\n",
    "                                 epsilon=epsilon * 0.1)\n",
    "            next_point = current_point - lam * g\n",
    "            if in_range(next_point, lower, upper):\n",
    "                dich_step_count += dsc\n",
    "                break\n",
    "            else:\n",
    "                lam_max /= 2.0\n",
    "        if np.linalg.norm(current_point - next_point) < epsilon:\n",
    "            if print_result:\n",
    "                print(\"Steps done:\", step_count)\n",
    "                print(\"Result: f(\", reduce(lambda a, b: \"{}, {}\".format(a, b), next_point), \") =  \",\n",
    "                      func(next_point)[0], sep=\"\")\n",
    "            if return_step_count:\n",
    "                return next_point, func(next_point), step_count, dich_step_count\n",
    "            else:\n",
    "                return next_point, func(next_point)\n",
    "        else:\n",
    "            current_point = next_point\n",
    "\n",
    "\n",
    "def dichotomy(left, right, func, epsilon=0.001):\n",
    "    step_count = 0\n",
    "    while right - left >= 2 * epsilon:\n",
    "        step_count += 1\n",
    "        mid = (left + right) / 2.0\n",
    "        delta = random() * (right - left) / 2.0\n",
    "        new_left = mid - delta\n",
    "        new_right = mid + delta\n",
    "        if func(new_left) > func(new_right):\n",
    "            left = new_left\n",
    "        else:\n",
    "            right = new_right\n",
    "    return (right + left) / 2, step_count\n",
    "\n",
    "\n",
    "def monte_carlo_hybrid(opt_method, *args, tries=100, **kwargs):\n",
    "    arg_min, _min = opt_method(*args, **kwargs)\n",
    "    for _ in range(tries - 1):\n",
    "        point, value = opt_method(*args, **kwargs)\n",
    "        if value < _min:\n",
    "            arg_min, _min = point, value\n",
    "    return arg_min, _min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вспомогательные функции для анализа экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def stats(results, true_min=0, mode=\"\"):\n",
    "    print(\"Average distance to the minimum: \", end=\"\")\n",
    "    print(sum([np.linalg.norm(res[0] - true_min) for res in results]) / len(results))\n",
    "    if mode != \"monte\":\n",
    "        print(\"Average steps done: \", end=\"\")\n",
    "        print(sum([res[2] for res in results]) / len(results))\n",
    "        if mode == \"opt\":\n",
    "            print(\"Average dichotomy steps done: \", end=\"\")\n",
    "            print(sum([res[3] for res in results]) / len(results))\n",
    "\n",
    "\n",
    "def global_vs_local(results, true_min=0):\n",
    "    global_count = 0\n",
    "    local_count = 0\n",
    "    for res in results:\n",
    "        if np.linalg.norm(res[0] - true_min) < 0.01:\n",
    "            global_count += 1\n",
    "        else:\n",
    "            local_count += 1\n",
    "    print(\"Stopped at local minimum: \", local_count / len(results) * 100, \"% of times\", sep=\"\")\n",
    "    print(\"Stopped at global minimum: \", global_count / len(results) * 100, \"% of times\", sep=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Исследуемые функции\n",
    "Функции Розенброка:\n",
    "\n",
    "$ f(x_1, x_2, ..., x_n) = \\sum\\limits_{i=1}^{n-1} [(1-x_i)^2 + 100(x_{i+1} - x_i^2)^2] $\n",
    "\n",
    "$ \\nabla f(x_1, x_2, ..., x_n) = \\begin{pmatrix}\n",
    "-2(1-x_1) - 400x_1(x_2 - x_1^2) \\\\\n",
    "200(x_2 - x_1^2) - 2(1-x_2) -400x_2(x_3 - x_2^2) \\\\\n",
    "... \\\\\n",
    "200(x_k - x_{k-1}^2) - 2(1-x_k) -400x_k(x_{k+1} - x_k^2) \\\\\n",
    "... \\\\\n",
    "200(x_{n-1} - x_{n-2}^2) - 2(1-x_{n-1}) -400x_{n-1}(x_{n} - x_{n-1}^2) \\\\\n",
    "200(x_n - x_{n-1}^2)\n",
    "\\end{pmatrix} $\n",
    "\n",
    "![rosenbrock](https://psv4.vk.me/c414626/u182545048/docs/2be9a0c9e5e5/figure_3.png?extra=6X5Gpv6qtohbbmmCw30Ge9glfCkg_lqljHvKFkNbDLa8CiSr3ogTIVfKVjy12oIzaEKIwCx1QVT_Ac4vcQcFvd5M97vbCmxErAJd3AXLy3gj_TnvyBtRvYsEG5zWP_Kz6kdFrd-iUIUY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def rosenbrock(vec):\n",
    "    return np.array([sum(map(lambda a: (1.0 - a[0]) ** 2 + 100.0 * (a[1] - a[0] ** 2) ** 2, zip(vec, vec[1:])))])\n",
    "\n",
    "\n",
    "def rosenbrock_grad(vec):\n",
    "    n = len(vec)\n",
    "    grad = [-2 * (1 - vec[0]) - 400 * vec[0] * (vec[1] - vec[0] ** 2)]\n",
    "    for k in range(1, n - 1):\n",
    "        grad.append(200 * (vec[k] - vec[k - 1] ** 2) - 2 * (1 - vec[k] - 400 * vec[k] * (vec[k + 1] - vec[k] ** 2)))\n",
    "    grad.append(200 * (vec[n - 1] - vec[n - 2] ** 2))\n",
    "    return np.array(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параболоид. Используется как легкая для нахождения минимума функция в целях проверки работоспособности алгоритмов, а также для демонстрации проблемы градиентного спуска в случае отсутствия в исследуемом множестве точек локального минимума:\n",
    "\n",
    "$ f(x,y) = x^2 + y^2 $\n",
    "\n",
    "$ \\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ 2y \\end{pmatrix} $\n",
    "\n",
    "![paraboloid](https://psv4.vk.me/c404626/u182545048/docs/6cb1453c8487/figure_5.png?extra=jYsKGnAtgW24NkSlAC0pAfHSeNuR_qPMZ6Jo5BttdN8EOS5-pOH2O9q7ofiFeeAgAZJp4ry3igdF1yo34a0LtKhyHjuhFqlRZYrK9iDjIotHM9NLfoRzp_VbvJu2SBKSqna1KzewOahk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def paraboloid(vec):\n",
    "    (x, y) = vec\n",
    "    return np.array([x ** 2 + y ** 2])\n",
    "\n",
    "\n",
    "def paraboloid_grad(vec):\n",
    "    (x, y) = vec\n",
    "    return np.array([2 * x, 2 * y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная функция используется для демонстрации проблемы достижения локального минимума на пути к глобальному, так как она имеет континуум точек локального минимума, окружающих точку глобального минимума:\n",
    "\n",
    "$ f(x,y) = 15e^{-(x^2+y^2)}(x^2+y^2)+0.4^6(x^2+y^2)^3 $\n",
    "\n",
    "$ \\nabla f(x,y) = \\begin{pmatrix}\n",
    "2x(15e^{-(x^2+y2)}(1-(x^2+y^2)) + 0.4^6 \\cdot 3(x^2+y^2)^2) \\\\\n",
    "2y(15e^{-(x^2+y2)}(1-(x^2+y^2)) + 0.4^6 \\cdot 3(x^2+y^2)^2)\n",
    "\\end{pmatrix} $\n",
    "\n",
    "![tricky](https://psv4.vk.me/c414626/u182545048/docs/ce23cf0c424c/figure_1.png?extra=3QspGl2V7EXZA-zlDq7QqEQdwWeONXTb3XiRrrxI5-rPDPJXrBZ4wXCwxg3IL2Reh6R4uMRRR4GldNw3T6fEYcr2IIlngdcwHJ1fLmjLAFdrLBFESGlYKLATL7j06DwLuKDhnGvz0KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def tricky(vec):\n",
    "    (x, y) = vec\n",
    "    return np.array(15.0 * math.exp(-(x ** 2 + y ** 2)) * (x ** 2 + y ** 2 - 0.4) + (0.4 ** 6) * (x ** 2 + y ** 2) ** 3)\n",
    "\n",
    "\n",
    "def tricky_grad(vec):\n",
    "    (x, y) = vec\n",
    "    return np.array([\n",
    "        (2 * x) * (15.0 * math.exp(-(x ** 2 + y ** 2)) * (1.0 - (x ** 2 + y ** 2 - 0.4)) +\n",
    "                   3 * (0.4 ** 6) * (x ** 2 + y ** 2)),\n",
    "        (2 * y) * (15.0 * math.exp(-(x ** 2 + y ** 2)) * (1.0 - (x ** 2 + y ** 2 - 0.4)) +\n",
    "                   3 * (0.4 ** 6) * (x ** 2 + y ** 2)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный спуск с постоянным шагом\n",
    "\n",
    "Градиентный спуск с постоянным шагом 0.01 неплохо справляется с задачей на параболоиде. Уменьшение величины $ \\varepsilon $\n",
    "на порядок приводит к уменьшению среднего расстояния между результатом и истинным минимумом также на порядок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.001\n",
      "Average distance to the minimum: 0.048495554615\n",
      "Average steps done: 210.94\n",
      "\n",
      "epsilon = 0.0001\n",
      "Average distance to the minimum: 0.00485881037756\n",
      "Average steps done: 320.68\n",
      "\n",
      "epsilon = 1e-05\n",
      "Average distance to the minimum: 0.000485736945462\n",
      "Average steps done: 444.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([-5, -5])\n",
    "__upper__ = np.array([5, 5])\n",
    "__eps__ = 0.001\n",
    "\n",
    "for _ in range(3):\n",
    "    print(\"epsilon =\", __eps__)\n",
    "    stats([descent(__lower__, __upper__, paraboloid, paraboloid_grad, constant_step, *[0.01], epsilon=__eps__,\n",
    "                   return_step_count=True) for _ in range(50)])\n",
    "    print()\n",
    "    __eps__ *= 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, на функции Розенброка градиентный спуск с шагом 0.01 не сойдется:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 1.12643589782\n",
      "Average steps done: 100001.0\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0])\n",
    "__upper__ = np.array([2, 2])\n",
    "\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, constant_step, *[0.01], epsilon=0.001,\n",
    "               return_step_count=True) for _ in range(3)], np.array([1, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При уменьшении шага до 0.001 метод сходится, но со значительной ошибкой. Уменьшение $ \\varepsilon $ приводит к улучшению результата ценой значительного увеличения числа шагов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.0001\n",
      "Average distance to the minimum: 0.217042248438\n",
      "Average steps done: 1533.14\n",
      "\n",
      "epsilon = 1e-05\n",
      "Average distance to the minimum: 0.0249625216827\n",
      "Average steps done: 6629.5\n",
      "\n",
      "epsilon = 1.0000000000000002e-06\n",
      "Average distance to the minimum: 0.00250155541479\n",
      "Average steps done: 11665.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0])\n",
    "__upper__ = np.array([2, 2])\n",
    "__true_min__ = np.array([1, 1])\n",
    "__eps__ = 0.0001\n",
    "\n",
    "for _ in range(3):\n",
    "    print(\"epsilon =\", __eps__)\n",
    "    stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, constant_step, *[0.001], epsilon=__eps__,\n",
    "                   return_step_count=True) for _ in range(50)], __true_min__)\n",
    "    print()\n",
    "    __eps__ *= 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На трехмерной функции Розенброка метод работает плохо, не приближаясь к точке минимума за десятки тысяч шагов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 1.30916142764\n",
      "Average steps done: 44004.98\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0, 0])\n",
    "__upper__ = np.array([2, 2, 2])\n",
    "__true_min__ = np.array([1, 1, 1])\n",
    "\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, constant_step, *[0.01], epsilon=0.0001,\n",
    "               return_step_count=True) for _ in range(50)], __true_min__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный спуск с уменьшающимся шагом\n",
    "\n",
    "На параболоиде отлично справляется с задачей при любых адекватных параметрах (не слишком маленький начальный шаг, не слишком быстрое уменьшение шага): за небольшое число шагов достигается высокая точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 2.42332172569e-06\n",
      "Average steps done: 28.04\n",
      "\n",
      "Average distance to the minimum: 6.32207992185e-06\n",
      "Average steps done: 19.6\n",
      "\n",
      "Average distance to the minimum: 6.68463005753e-06\n",
      "Average steps done: 14.02\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([-5, -5])\n",
    "__upper__ = np.array([5, 5])\n",
    "__eps__ = 0.0001\n",
    "stats([descent(__lower__, __upper__, paraboloid, paraboloid_grad, decreasing_step, *[2, 0.95], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)])\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, paraboloid, paraboloid_grad, decreasing_step, *[1, 0.97], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)])\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, paraboloid, paraboloid_grad, decreasing_step, *[1.5, 0.9], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На сложных для оптимизации функциях, в частности, на функции Розенброка, метод становится крайне чувствителен к подбору параметров, и выдает крайне неточные результаты при очень небольших значениях $ \\varepsilon $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 0.513264293185\n",
      "Average steps done: 2586.3\n",
      "\n",
      "Average distance to the minimum: 0.689497062207\n",
      "Average steps done: 1365.86\n",
      "\n",
      "Average distance to the minimum: 0.876957449092\n",
      "Average steps done: 669.98\n",
      "\n",
      "Average distance to the minimum: 1.01492501447\n",
      "Average steps done: 286.0\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0])\n",
    "__upper__ = np.array([2, 2])\n",
    "__true_min__ = np.array([1, 1])\n",
    "__eps__ = 0.000001\n",
    "\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.7, 0.995], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.7, 0.99], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.6, 0.98], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.3, 0.95], epsilon=__eps__,\n",
    "               return_step_count=True) for _ in range(50)], __true_min__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К значительному улучшению результатов использования данного метода при сохранении остальных параметров приводит нормализация вектора градиента. В случае с функцией Розенброка, градиент которой в окрестности точки $(1, 1)$ невелик по модулю, его нормализация гарантирует, что процесс не закончится слишком быстро. В целом, использование нормированного градиента может облегчить подбор параметров, в частности, для функций, быстро возрастающих при небольшом удалении от точки минимума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 0.0466913077546\n",
      "Average steps done: 2839.44\n",
      "\n",
      "Average distance to the minimum: 0.0457466897647\n",
      "Average steps done: 1417.56\n",
      "\n",
      "Average distance to the minimum: 0.144399775371\n",
      "Average steps done: 705.12\n",
      "\n",
      "Average distance to the minimum: 0.530960538034\n",
      "Average steps done: 282.4\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0])\n",
    "__upper__ = np.array([2, 2])\n",
    "__true_min__ = np.array([1, 1])\n",
    "__eps__ = 0.000001\n",
    "\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.7, 0.995], epsilon=0.000001,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.7, 0.99], epsilon=0.000001,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.6, 0.98], epsilon=0.000001,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[2.3, 0.95], epsilon=0.000001,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На трехмерной функции Розеброка метод проявляет себя плохо (хотя нельзя исключать, что я подобрала слишком плохие параметры):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 1.23453611505\n",
      "Average steps done: 25.52\n",
      "\n",
      "Average distance to the minimum: 1.25295156815\n",
      "Average steps done: 22.76\n",
      "\n",
      "Average distance to the minimum: 1.2063140146\n",
      "Average steps done: 18.26\n",
      "\n",
      "Average distance to the minimum: 1.20175153607\n",
      "Average steps done: 24.56\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0, 0])\n",
    "__upper__ = np.array([2, 2, 2])\n",
    "__true_min__ = np.array([1, 1, 1])\n",
    "__eps__ = 0.000001\n",
    "\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[5, 0.99], epsilon=__eps__,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[4, 0.99], epsilon=__eps__,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[3.5, 0.98], epsilon=__eps__,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n",
    "print()\n",
    "stats([descent(__lower__, __upper__, rosenbrock, rosenbrock_grad, decreasing_step, *[3, 0.95], epsilon=__eps__,\n",
    "               normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный спуск с оптимальным шагом\n",
    "\n",
    "На параболоиде справляется с задачей отлично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 1.34188025395e-10\n",
      "Average steps done: 3.0\n",
      "Average dichotomy steps done: 87.66\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([-5, -5])\n",
    "__upper__ = np.array([5, 5])\n",
    "\n",
    "stats([descent_optimal_step(__lower__, __upper__, paraboloid, paraboloid_grad, epsilon=0.0001,\n",
    "                            return_step_count=True) for _ in range(50)], mode=\"opt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На функции Розенброка возникают проблемы как с точностью, так и довольно большим количеством шагов. Нормализация вектора улучшает ситуацию довольно значительно, но недостаточно. Однако, следует учитывать, что результаты с точностью того же порядка получались градиентным спуском с уменьшающимся шагом с параметрами, совсем немного отличающихся от тех, что давали лучший результат, при значении $ \\varepsilon $ на два порядка меньше, чем в следующем эксперименте. Таким образом, данный метод справляется с задачей хорошо, и градиентный спуск с уменьшающимся шагом делает это лучше только при очень тщательном подборе параметров, которого не требуется для данного метода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 0.0715177648239\n",
      "Average steps done: 1004.34\n",
      "Average dichotomy steps done: 35069.32\n",
      "\n",
      "Average distance to the minimum: 0.0453260702239\n",
      "Average steps done: 294.8\n",
      "Average dichotomy steps done: 10371.76\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0])\n",
    "__upper__ = np.array([2, 2])\n",
    "__true_min__ = np.array([1, 1])\n",
    "__eps__ = 0.0001\n",
    "\n",
    "stats([descent_optimal_step(__lower__, __upper__, rosenbrock, rosenbrock_grad, epsilon=__eps__,\n",
    "                            return_step_count=True) for _ in range(50)], __true_min__, mode=\"opt\")\n",
    "print()\n",
    "stats([descent_optimal_step(__lower__, __upper__, rosenbrock, rosenbrock_grad, epsilon=__eps__,\n",
    "                            normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__, mode=\"opt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На трехмерной функции Розенброка метод дает низкую степень точности, которая не повышается уменьшением величины $ \\varepsilon $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.0001\n",
      "Average distance to the minimum: 0.951456578235\n",
      "Average steps done: 8.24\n",
      "Average dichotomy steps done: 216.02\n",
      "\n",
      "epsilon = 1e-05\n",
      "Average distance to the minimum: 0.857404962326\n",
      "Average steps done: 6.14\n",
      "Average dichotomy steps done: 199.32\n",
      "\n",
      "epsilon = 1.0000000000000002e-06\n",
      "Average distance to the minimum: 0.811984219188\n",
      "Average steps done: 12.34\n",
      "Average dichotomy steps done: 474.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0, 0])\n",
    "__upper__ = np.array([2, 2, 2])\n",
    "__true_min__ = np.array([1, 1, 1])\n",
    "__eps__ = 0.0001\n",
    "\n",
    "for _ in range(3):\n",
    "    print(\"epsilon =\", __eps__)\n",
    "    stats([descent_optimal_step(__lower__, __upper__, rosenbrock, rosenbrock_grad, epsilon=__eps__,\n",
    "                                normalize_grad=True, return_step_count=True) for _ in range(50)], __true_min__, mode=\"opt\")\n",
    "    print()\n",
    "    __eps__ *= 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общей проблемой для всех модификаций градиентного спуска является то, что если процесс на пути к точке глобального минимума попадет в локальный, то он не выходит из нее. Это хорошо видно на последней из выбранных мной для рассмотрения функций, имеющей континуум локальных минимумов, окружающих глобальный:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant step\n",
      "Stopped at local minimum: 90.0% of times\n",
      "Stopped at global minimum: 10.0% of times\n",
      "\n",
      "Decreasing step\n",
      "Stopped at local minimum: 10.0% of times\n",
      "Stopped at global minimum: 90.0% of times\n",
      "\n",
      "Optimal step\n",
      "Stopped at local minimum: 72.0% of times\n",
      "Stopped at global minimum: 28.000000000000004% of times\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([-3, -3])\n",
    "__upper__ = np.array([3, 3])\n",
    "__eps__ = 0.0001\n",
    "\n",
    "print(\"Constant step\")\n",
    "global_vs_local([descent(__lower__, __upper__, tricky, tricky_grad, constant_step, *[0.01], epsilon=__eps__,\n",
    "                         return_step_count=True) for _ in range(100)])\n",
    "print(\"\\nDecreasing step\")\n",
    "global_vs_local([descent(__lower__, __upper__, tricky, tricky_grad, decreasing_step, *[9, 0.99], epsilon=__eps__,\n",
    "                         normalize_grad=True, return_step_count=True) for _ in range(10)])\n",
    "print(\"\\nOptimal step\")\n",
    "global_vs_local([descent_optimal_step(__lower__, __upper__, tricky, tricky_grad, epsilon=__eps__,\n",
    "                                      return_step_count=True) for _ in range(100)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гибридный подход, совмещающий градиентный спуск с методом Монте-Карло, решает эту проблему:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant step\n",
      "Stopped at local minimum: 0.0% of times\n",
      "Stopped at global minimum: 100.0% of times\n",
      "\n",
      "Decreasing step\n",
      "Stopped at local minimum: 0.0% of times\n",
      "Stopped at global minimum: 100.0% of times\n",
      "\n",
      "Optimal step\n",
      "Stopped at local minimum: 0.0% of times\n",
      "Stopped at global minimum: 100.0% of times\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([-3, -3])\n",
    "__upper__ = np.array([3, 3])\n",
    "__eps__ = 0.0001\n",
    "\n",
    "print(\"Constant step\")\n",
    "global_vs_local([monte_carlo_hybrid(descent, __lower__, __upper__, tricky, tricky_grad, constant_step, *[0.01],\n",
    "                                    tries=100, epsilon=__eps__) for _ in range(10)])\n",
    "print(\"\\nDecreasing step\")\n",
    "global_vs_local([monte_carlo_hybrid(descent, __lower__, __upper__, tricky, tricky_grad, decreasing_step, *[9, 0.99],\n",
    "                                    tries=100, normalize_grad=True, epsilon=__eps__) for _ in range(10)])\n",
    "print(\"\\nOptimal step\")\n",
    "global_vs_local([monte_carlo_hybrid(descent_optimal_step, __lower__, __upper__, tricky, tricky_grad, tries=100,\n",
    "                                    epsilon=__eps__) for _ in range(10)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, использование гибридного алгоритма значительно улучшает результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 0.000224994110241\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0])\n",
    "__upper__ = np.array([2, 2])\n",
    "__true_min__ = np.array([1, 1])\n",
    "\n",
    "stats([monte_carlo_hybrid(descent_optimal_step, __lower__, __upper__, rosenbrock, rosenbrock_grad, tries=100,\n",
    "                                epsilon=0.0001, normalize_grad=True) for _ in range(10)], __true_min__, mode=\"monte\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 0.712892252433\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([0, 0, 0])\n",
    "__upper__ = np.array([2, 2, 2])\n",
    "__true_min__ = np.array([1, 1, 1])\n",
    "\n",
    "stats([monte_carlo_hybrid(descent_optimal_step, __lower__, __upper__, rosenbrock, rosenbrock_grad, tries=100,\n",
    "                                epsilon=0.0001, normalize_grad=True) for _ in range(10)], __true_min__, mode=\"monte\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, это происходит ценой значительного увеличения числа операций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другая проблема градиентного спуска проявляет себя в случае, если на исследуемом множестве функция не имеет точек локального минимума. Двигаясь в направлении ближайшей такой точки, алгоритм \"упирается\" в границу и продолжает попытки совершать небольшие шаги в том направлении, пока не выполнится условие завершения работы. Это легко продемонстировать на следующем примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance to the minimum: 0.612220226098\n",
      "Average steps done: 9.22\n",
      "Average dichotomy steps done: 154.86\n"
     ]
    }
   ],
   "source": [
    "__lower__ = np.array([1, 2])\n",
    "__upper__ = np.array([3, 4])\n",
    "__true_min__ = __lower__\n",
    "\n",
    "stats([descent_optimal_step(__lower__, __upper__, paraboloid, paraboloid_grad, epsilon=0.0001, normalize_grad=True,\n",
    "                                return_step_count=True) for _ in range(50)], __true_min__, mode=\"opt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Выводы\n",
    "1. Градиентный спуск с уменьшающимся шагом и градиентный спуск с оптимальным шагом проявляют себя лучше, если нормализовать вектор градиента.\n",
    "2. Метод с уменьшающимся шагом может проявлять себя лучше метода с оптимальным шагом при хорошем подборе параметров. Однако, метод с оптимальным шагом не требует подбирать параметры вручную, что, безусловно, является его преимуществом.\n",
    "3. Проблема достижения локального минимума на пути к глобальному решается совместным использованием градиентного спуска и некоторого стохастического метода, например, метода Монте-Карло. Использование гибридного алгоритма в целом приводит к повышению точности результата, в том числе на сложных для нахождения минимума функциях.\n",
    "4. Существует проблема с нахождением минимума на множестве, где нет точек локального минимума. По всей видимости, для ее разрешения необходимо проверять граничные точки.\n",
    "5. Градиентный спуск проявляет себя хуже с увеличением размерности пространства."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
